
/// Takes flonum from stack and boxes it into a heap object

#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpBoxFlonum {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpBoxFlonum {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpBoxFlonum {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpBoxFlonum {
    pub const OPCODE_ID: OpcodeID = OP_BOX_FLONUM;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**box-flonum"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_BOX_FLONUM_DST_INDEX: usize = 0;
pub const OP_BOX_FLONUM_SRC_INDEX: usize = 1;


/// Unboxes heap object into flonum and stores it in `dst`

#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpUnboxFlonum {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpUnboxFlonum {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpUnboxFlonum {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpUnboxFlonum {
    pub const OPCODE_ID: OpcodeID = OP_UNBOX_FLONUM;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**unbox-flonum"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_UNBOX_FLONUM_DST_INDEX: usize = 0;
pub const OP_UNBOX_FLONUM_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsHeapObject {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpIsHeapObject {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsHeapObject {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsHeapObject {
    pub const OPCODE_ID: OpcodeID = OP_IS_HEAP_OBJECT;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-heap-object"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_IS_HEAP_OBJECT_DST_INDEX: usize = 0;
pub const OP_IS_HEAP_OBJECT_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsSmi {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpIsSmi {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsSmi {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsSmi {
    pub const OPCODE_ID: OpcodeID = OP_IS_SMI;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-smi"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_IS_SMI_DST_INDEX: usize = 0;
pub const OP_IS_SMI_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpAllocateUntaggedKnown {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub size: usize,
 
}
impl std::ops::Deref for OpAllocateUntaggedKnown {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpAllocateUntaggedKnown {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpAllocateUntaggedKnown {
    pub const OPCODE_ID: OpcodeID = OP_ALLOCATE_UNTAGGED_KNOWN;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
size: Fits::<usize, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
size: Fits::<usize, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
size: Fits::<usize, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_size<F>(&mut self, value: usize, f: F)
        where F: FnOnce() -> usize {
            if self.is_wide32() { self.set_size_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_size_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_size_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_size_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: usize, f: F) 
        where F: FnOnce() -> usize {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<usize, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<usize, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<usize, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<usize, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<usize, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<usize, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: usize,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<usize, {OpcodeSize::Narrow}>::check(size),
    OpcodeSize::Wide16 => Fits::<usize, {OpcodeSize::Wide16}>::check(size),
    OpcodeSize::Wide32 => Fits::<usize, {OpcodeSize::Wide32}>::check(size)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: usize,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, size, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<usize, {OpcodeSize::Narrow}>::convert(size)),
    OpcodeSize::Wide16 => gen.write(Fits::<usize, {OpcodeSize::Wide16}>::convert(size)),
    OpcodeSize::Wide32 => gen.write(Fits::<usize, {OpcodeSize::Wide32}>::convert(size))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: usize,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, size, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: usize,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, size, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: usize,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, size, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: usize,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, size, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: usize,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, size, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: usize,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, size, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, size, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, size, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**allocate-untagged-known"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("size", self.size, false);

}

}

pub const OP_ALLOCATE_UNTAGGED_KNOWN_DST_INDEX: usize = 0;
pub const OP_ALLOCATE_UNTAGGED_KNOWN_SIZE_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpAllocateUntagged {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub size: VirtualRegister,
 
}
impl std::ops::Deref for OpAllocateUntagged {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpAllocateUntagged {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpAllocateUntagged {
    pub const OPCODE_ID: OpcodeID = OP_ALLOCATE_UNTAGGED;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
size: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
size: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
size: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_size<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_size_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_size_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_size_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_size_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(size),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(size),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(size)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, size, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(size)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(size)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(size))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, size, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, size, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, size, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, size, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, size, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, size, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, size, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, size, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**allocate-untagged"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("size", self.size, false);

}

}

pub const OP_ALLOCATE_UNTAGGED_DST_INDEX: usize = 0;
pub const OP_ALLOCATE_UNTAGGED_SIZE_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpAllocateKnown {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub size: usize,
    pub tag: u16,
 
}
impl std::ops::Deref for OpAllocateKnown {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpAllocateKnown {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpAllocateKnown {
    pub const OPCODE_ID: OpcodeID = OP_ALLOCATE_KNOWN;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
size: Fits::<usize, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
tag: Fits::<u16, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
size: Fits::<usize, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
tag: Fits::<u16, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
size: Fits::<usize, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
tag: Fits::<u16, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_size<F>(&mut self, value: usize, f: F)
        where F: FnOnce() -> usize {
            if self.is_wide32() { self.set_size_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_size_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_size_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_size_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: usize, f: F) 
        where F: FnOnce() -> usize {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<usize, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<usize, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<usize, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<usize, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<usize, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<usize, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_tag<F>(&mut self, value: u16, f: F)
        where F: FnOnce() -> u16 {
            if self.is_wide32() { self.set_tag_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_tag_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_tag_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_tag_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: u16, f: F) 
        where F: FnOnce() -> u16 {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<u16, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<u16, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<u16, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<u16, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<u16, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<u16, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: usize,tag: u16,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<usize, {OpcodeSize::Narrow}>::check(size),
    OpcodeSize::Wide16 => Fits::<usize, {OpcodeSize::Wide16}>::check(size),
    OpcodeSize::Wide32 => Fits::<usize, {OpcodeSize::Wide32}>::check(size)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<u16, {OpcodeSize::Narrow}>::check(tag),
    OpcodeSize::Wide16 => Fits::<u16, {OpcodeSize::Wide16}>::check(tag),
    OpcodeSize::Wide32 => Fits::<u16, {OpcodeSize::Wide32}>::check(tag)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: usize,tag: u16,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, size, tag, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<usize, {OpcodeSize::Narrow}>::convert(size)),
    OpcodeSize::Wide16 => gen.write(Fits::<usize, {OpcodeSize::Wide16}>::convert(size)),
    OpcodeSize::Wide32 => gen.write(Fits::<usize, {OpcodeSize::Wide32}>::convert(size))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<u16, {OpcodeSize::Narrow}>::convert(tag)),
    OpcodeSize::Wide16 => gen.write(Fits::<u16, {OpcodeSize::Wide16}>::convert(tag)),
    OpcodeSize::Wide32 => gen.write(Fits::<u16, {OpcodeSize::Wide32}>::convert(tag))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: usize,tag: u16,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, size, tag, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: usize,tag: u16,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, size, tag, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: usize,tag: u16,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, size, tag, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: usize,tag: u16,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, size, tag, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: usize,tag: u16,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, size, tag, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: usize,tag: u16,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, size, tag, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, size, tag, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, size, tag, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**allocate-known"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("size", self.size, false);
dumper.dump_operand("tag", self.tag, false);

}

}

pub const OP_ALLOCATE_KNOWN_DST_INDEX: usize = 0;
pub const OP_ALLOCATE_KNOWN_SIZE_INDEX: usize = 1;
pub const OP_ALLOCATE_KNOWN_TAG_INDEX: usize = 2;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpAllocate {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub size: VirtualRegister,
    pub tag: u16,
 
}
impl std::ops::Deref for OpAllocate {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpAllocate {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpAllocate {
    pub const OPCODE_ID: OpcodeID = OP_ALLOCATE;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
size: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
tag: Fits::<u16, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
size: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
tag: Fits::<u16, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
size: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
tag: Fits::<u16, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_size<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_size_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_size_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_size_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_size_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_tag<F>(&mut self, value: u16, f: F)
        where F: FnOnce() -> u16 {
            if self.is_wide32() { self.set_tag_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_tag_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_tag_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_tag_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: u16, f: F) 
        where F: FnOnce() -> u16 {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<u16, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<u16, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<u16, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<u16, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<u16, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<u16, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: VirtualRegister,tag: u16,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(size),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(size),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(size)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<u16, {OpcodeSize::Narrow}>::check(tag),
    OpcodeSize::Wide16 => Fits::<u16, {OpcodeSize::Wide16}>::check(tag),
    OpcodeSize::Wide32 => Fits::<u16, {OpcodeSize::Wide32}>::check(tag)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: VirtualRegister,tag: u16,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, size, tag, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(size)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(size)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(size))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<u16, {OpcodeSize::Narrow}>::convert(tag)),
    OpcodeSize::Wide16 => gen.write(Fits::<u16, {OpcodeSize::Wide16}>::convert(tag)),
    OpcodeSize::Wide32 => gen.write(Fits::<u16, {OpcodeSize::Wide32}>::convert(tag))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: VirtualRegister,tag: u16,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, size, tag, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: VirtualRegister,tag: u16,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, size, tag, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: VirtualRegister,tag: u16,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, size, tag, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: VirtualRegister,tag: u16,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, size, tag, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: VirtualRegister,tag: u16,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, size, tag, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,size: VirtualRegister,tag: u16,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, size, tag, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, size, tag, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, size, tag, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**allocate"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("size", self.size, false);
dumper.dump_operand("tag", self.tag, false);

}

}

pub const OP_ALLOCATE_DST_INDEX: usize = 0;
pub const OP_ALLOCATE_SIZE_INDEX: usize = 1;
pub const OP_ALLOCATE_TAG_INDEX: usize = 2;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpScmSetById {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub index: usize,
    pub value: VirtualRegister,
 
}
impl std::ops::Deref for OpScmSetById {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpScmSetById {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpScmSetById {
    pub const OPCODE_ID: OpcodeID = OP_SCM_SET_BY_ID;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
index: Fits::<usize, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
value: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
index: Fits::<usize, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
value: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
index: Fits::<usize, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
value: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_index<F>(&mut self, value: usize, f: F)
        where F: FnOnce() -> usize {
            if self.is_wide32() { self.set_index_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_index_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_index_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_index_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: usize, f: F) 
        where F: FnOnce() -> usize {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<usize, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<usize, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<usize, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<usize, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<usize, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<usize, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_value<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_value_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_value_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_value_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_value_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: usize,value: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<usize, {OpcodeSize::Narrow}>::check(index),
    OpcodeSize::Wide16 => Fits::<usize, {OpcodeSize::Wide16}>::check(index),
    OpcodeSize::Wide32 => Fits::<usize, {OpcodeSize::Wide32}>::check(index)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: usize,value: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, index, value, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<usize, {OpcodeSize::Narrow}>::convert(index)),
    OpcodeSize::Wide16 => gen.write(Fits::<usize, {OpcodeSize::Wide16}>::convert(index)),
    OpcodeSize::Wide32 => gen.write(Fits::<usize, {OpcodeSize::Wide32}>::convert(index))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: usize,value: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, index, value, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: usize,value: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, index, value, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: usize,value: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, index, value, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: usize,value: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, index, value, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: usize,value: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, index, value, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: usize,value: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, index, value, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, index, value, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, index, value, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**scm-set-by-id"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("index", self.index, false);
dumper.dump_operand("value", self.value, false);

}

}

pub const OP_SCM_SET_BY_ID_DST_INDEX: usize = 0;
pub const OP_SCM_SET_BY_ID_INDEX_INDEX: usize = 1;
pub const OP_SCM_SET_BY_ID_VALUE_INDEX: usize = 2;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpScmRefById {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
    pub index: usize,
 
}
impl std::ops::Deref for OpScmRefById {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpScmRefById {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpScmRefById {
    pub const OPCODE_ID: OpcodeID = OP_SCM_REF_BY_ID;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
index: Fits::<usize, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
index: Fits::<usize, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
index: Fits::<usize, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_index<F>(&mut self, value: usize, f: F)
        where F: FnOnce() -> usize {
            if self.is_wide32() { self.set_index_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_index_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_index_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_index_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: usize, f: F) 
        where F: FnOnce() -> usize {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<usize, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<usize, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<usize, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<usize, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<usize, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<usize, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: usize,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<usize, {OpcodeSize::Narrow}>::check(index),
    OpcodeSize::Wide16 => Fits::<usize, {OpcodeSize::Wide16}>::check(index),
    OpcodeSize::Wide32 => Fits::<usize, {OpcodeSize::Wide32}>::check(index)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: usize,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, index, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<usize, {OpcodeSize::Narrow}>::convert(index)),
    OpcodeSize::Wide16 => gen.write(Fits::<usize, {OpcodeSize::Wide16}>::convert(index)),
    OpcodeSize::Wide32 => gen.write(Fits::<usize, {OpcodeSize::Wide32}>::convert(index))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: usize,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, index, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: usize,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, index, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: usize,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, index, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: usize,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, index, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: usize,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, index, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: usize,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, index, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, index, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, index, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**scm-ref-by-id"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);
dumper.dump_operand("index", self.index, false);

}

}

pub const OP_SCM_REF_BY_ID_DST_INDEX: usize = 0;
pub const OP_SCM_REF_BY_ID_SRC_INDEX: usize = 1;
pub const OP_SCM_REF_BY_ID_INDEX_INDEX: usize = 2;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpScmSet {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub index: VirtualRegister,
    pub value: VirtualRegister,
 
}
impl std::ops::Deref for OpScmSet {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpScmSet {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpScmSet {
    pub const OPCODE_ID: OpcodeID = OP_SCM_SET;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
index: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
value: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
index: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
value: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
index: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
value: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_index<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_index_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_index_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_index_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_index_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_value<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_value_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_value_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_value_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_value_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: VirtualRegister,value: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(index),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(index),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(index)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: VirtualRegister,value: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, index, value, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(index)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(index)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(index))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: VirtualRegister,value: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, index, value, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: VirtualRegister,value: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, index, value, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: VirtualRegister,value: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, index, value, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: VirtualRegister,value: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, index, value, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: VirtualRegister,value: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, index, value, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: VirtualRegister,value: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, index, value, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, index, value, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, index, value, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**scm-set"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("index", self.index, false);
dumper.dump_operand("value", self.value, false);

}

}

pub const OP_SCM_SET_DST_INDEX: usize = 0;
pub const OP_SCM_SET_INDEX_INDEX: usize = 1;
pub const OP_SCM_SET_VALUE_INDEX: usize = 2;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpScmRef {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
    pub index: VirtualRegister,
 
}
impl std::ops::Deref for OpScmRef {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpScmRef {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpScmRef {
    pub const OPCODE_ID: OpcodeID = OP_SCM_REF;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
index: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
index: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
index: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_index<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_index_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_index_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_index_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_index_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(index),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(index),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(index)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, index, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(index)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(index)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(index))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, index, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, index, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, index, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, index, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, index, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, index, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, index, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, index, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**scm-ref"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);
dumper.dump_operand("index", self.index, false);

}

}

pub const OP_SCM_REF_DST_INDEX: usize = 0;
pub const OP_SCM_REF_SRC_INDEX: usize = 1;
pub const OP_SCM_REF_INDEX_INDEX: usize = 2;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpScmSetWordById {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub index: usize,
    pub value: VirtualRegister,
 
}
impl std::ops::Deref for OpScmSetWordById {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpScmSetWordById {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpScmSetWordById {
    pub const OPCODE_ID: OpcodeID = OP_SCM_SET_WORD_BY_ID;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
index: Fits::<usize, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
value: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
index: Fits::<usize, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
value: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
index: Fits::<usize, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
value: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_index<F>(&mut self, value: usize, f: F)
        where F: FnOnce() -> usize {
            if self.is_wide32() { self.set_index_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_index_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_index_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_index_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: usize, f: F) 
        where F: FnOnce() -> usize {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<usize, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<usize, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<usize, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<usize, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<usize, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<usize, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_value<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_value_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_value_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_value_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_value_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: usize,value: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<usize, {OpcodeSize::Narrow}>::check(index),
    OpcodeSize::Wide16 => Fits::<usize, {OpcodeSize::Wide16}>::check(index),
    OpcodeSize::Wide32 => Fits::<usize, {OpcodeSize::Wide32}>::check(index)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: usize,value: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, index, value, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<usize, {OpcodeSize::Narrow}>::convert(index)),
    OpcodeSize::Wide16 => gen.write(Fits::<usize, {OpcodeSize::Wide16}>::convert(index)),
    OpcodeSize::Wide32 => gen.write(Fits::<usize, {OpcodeSize::Wide32}>::convert(index))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: usize,value: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, index, value, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: usize,value: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, index, value, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: usize,value: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, index, value, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: usize,value: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, index, value, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: usize,value: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, index, value, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: usize,value: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, index, value, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, index, value, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, index, value, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**scm-set-word-by-id"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("index", self.index, false);
dumper.dump_operand("value", self.value, false);

}

}

pub const OP_SCM_SET_WORD_BY_ID_DST_INDEX: usize = 0;
pub const OP_SCM_SET_WORD_BY_ID_INDEX_INDEX: usize = 1;
pub const OP_SCM_SET_WORD_BY_ID_VALUE_INDEX: usize = 2;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpScmRefWordById {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
    pub index: usize,
 
}
impl std::ops::Deref for OpScmRefWordById {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpScmRefWordById {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpScmRefWordById {
    pub const OPCODE_ID: OpcodeID = OP_SCM_REF_WORD_BY_ID;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
index: Fits::<usize, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
index: Fits::<usize, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
index: Fits::<usize, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_index<F>(&mut self, value: usize, f: F)
        where F: FnOnce() -> usize {
            if self.is_wide32() { self.set_index_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_index_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_index_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_index_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: usize, f: F) 
        where F: FnOnce() -> usize {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<usize, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<usize, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<usize, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<usize, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<usize, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<usize, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: usize,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<usize, {OpcodeSize::Narrow}>::check(index),
    OpcodeSize::Wide16 => Fits::<usize, {OpcodeSize::Wide16}>::check(index),
    OpcodeSize::Wide32 => Fits::<usize, {OpcodeSize::Wide32}>::check(index)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: usize,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, index, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<usize, {OpcodeSize::Narrow}>::convert(index)),
    OpcodeSize::Wide16 => gen.write(Fits::<usize, {OpcodeSize::Wide16}>::convert(index)),
    OpcodeSize::Wide32 => gen.write(Fits::<usize, {OpcodeSize::Wide32}>::convert(index))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: usize,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, index, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: usize,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, index, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: usize,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, index, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: usize,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, index, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: usize,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, index, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: usize,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, index, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, index, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, index, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**scm-ref-word-by-id"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);
dumper.dump_operand("index", self.index, false);

}

}

pub const OP_SCM_REF_WORD_BY_ID_DST_INDEX: usize = 0;
pub const OP_SCM_REF_WORD_BY_ID_SRC_INDEX: usize = 1;
pub const OP_SCM_REF_WORD_BY_ID_INDEX_INDEX: usize = 2;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpScmSetWord {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub index: VirtualRegister,
    pub value: VirtualRegister,
 
}
impl std::ops::Deref for OpScmSetWord {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpScmSetWord {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpScmSetWord {
    pub const OPCODE_ID: OpcodeID = OP_SCM_SET_WORD;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
index: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
value: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
index: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
value: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
index: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
value: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_index<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_index_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_index_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_index_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_index_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_value<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_value_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_value_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_value_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_value_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: VirtualRegister,value: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(index),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(index),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(index)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: VirtualRegister,value: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, index, value, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(index)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(index)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(index))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: VirtualRegister,value: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, index, value, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: VirtualRegister,value: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, index, value, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: VirtualRegister,value: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, index, value, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: VirtualRegister,value: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, index, value, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: VirtualRegister,value: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, index, value, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,index: VirtualRegister,value: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, index, value, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, index, value, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, index, value, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**scm-set-word"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("index", self.index, false);
dumper.dump_operand("value", self.value, false);

}

}

pub const OP_SCM_SET_WORD_DST_INDEX: usize = 0;
pub const OP_SCM_SET_WORD_INDEX_INDEX: usize = 1;
pub const OP_SCM_SET_WORD_VALUE_INDEX: usize = 2;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpScmRefWord {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
    pub index: VirtualRegister,
 
}
impl std::ops::Deref for OpScmRefWord {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpScmRefWord {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpScmRefWord {
    pub const OPCODE_ID: OpcodeID = OP_SCM_REF_WORD;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
index: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
index: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
index: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_index<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_index_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_index_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_index_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_index_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(index),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(index),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(index)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, index, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(index)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(index)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(index))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, index, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, index, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, index, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, index, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, index, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,index: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, index, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, index, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, index, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**scm-ref-word"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);
dumper.dump_operand("index", self.index, false);

}

}

pub const OP_SCM_REF_WORD_DST_INDEX: usize = 0;
pub const OP_SCM_REF_WORD_SRC_INDEX: usize = 1;
pub const OP_SCM_REF_WORD_INDEX_INDEX: usize = 2;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsPair {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpIsPair {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsPair {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsPair {
    pub const OPCODE_ID: OpcodeID = OP_IS_PAIR;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-pair"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_IS_PAIR_DST_INDEX: usize = 0;
pub const OP_IS_PAIR_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsNull {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpIsNull {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsNull {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsNull {
    pub const OPCODE_ID: OpcodeID = OP_IS_NULL;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-null"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_IS_NULL_DST_INDEX: usize = 0;
pub const OP_IS_NULL_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsVector {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpIsVector {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsVector {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsVector {
    pub const OPCODE_ID: OpcodeID = OP_IS_VECTOR;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-vector"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_IS_VECTOR_DST_INDEX: usize = 0;
pub const OP_IS_VECTOR_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsString {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpIsString {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsString {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsString {
    pub const OPCODE_ID: OpcodeID = OP_IS_STRING;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-string"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_IS_STRING_DST_INDEX: usize = 0;
pub const OP_IS_STRING_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsSymbol {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpIsSymbol {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsSymbol {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsSymbol {
    pub const OPCODE_ID: OpcodeID = OP_IS_SYMBOL;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-symbol"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_IS_SYMBOL_DST_INDEX: usize = 0;
pub const OP_IS_SYMBOL_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsTuple {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpIsTuple {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsTuple {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsTuple {
    pub const OPCODE_ID: OpcodeID = OP_IS_TUPLE;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-tuple"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_IS_TUPLE_DST_INDEX: usize = 0;
pub const OP_IS_TUPLE_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsProcedure {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpIsProcedure {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsProcedure {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsProcedure {
    pub const OPCODE_ID: OpcodeID = OP_IS_PROCEDURE;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-procedure"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_IS_PROCEDURE_DST_INDEX: usize = 0;
pub const OP_IS_PROCEDURE_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsBoolean {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpIsBoolean {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsBoolean {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsBoolean {
    pub const OPCODE_ID: OpcodeID = OP_IS_BOOLEAN;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-boolean"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_IS_BOOLEAN_DST_INDEX: usize = 0;
pub const OP_IS_BOOLEAN_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsChar {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpIsChar {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsChar {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsChar {
    pub const OPCODE_ID: OpcodeID = OP_IS_CHAR;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-char"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_IS_CHAR_DST_INDEX: usize = 0;
pub const OP_IS_CHAR_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsUndefined {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpIsUndefined {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsUndefined {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsUndefined {
    pub const OPCODE_ID: OpcodeID = OP_IS_UNDEFINED;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-undefined"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_IS_UNDEFINED_DST_INDEX: usize = 0;
pub const OP_IS_UNDEFINED_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsUnspecified {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpIsUnspecified {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsUnspecified {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsUnspecified {
    pub const OPCODE_ID: OpcodeID = OP_IS_UNSPECIFIED;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-unspecified"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_IS_UNSPECIFIED_DST_INDEX: usize = 0;
pub const OP_IS_UNSPECIFIED_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsEofObject {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpIsEofObject {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsEofObject {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsEofObject {
    pub const OPCODE_ID: OpcodeID = OP_IS_EOF_OBJECT;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-eof-object"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_IS_EOF_OBJECT_DST_INDEX: usize = 0;
pub const OP_IS_EOF_OBJECT_SRC_INDEX: usize = 1;


/// Switch over SMI value in `src`.
/// `jtable` must be a vector of SMIs indicating an offset
/// to jump to, `default` is where we go if no entry in jtable is present

#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpSwitchImm {
    pub base: BaseInstruction,
    pub offset: isize,
    pub src: VirtualRegister,
    pub jtable: VirtualRegister,
    pub default: isize,
 
}
impl std::ops::Deref for OpSwitchImm {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpSwitchImm {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpSwitchImm {
    pub const OPCODE_ID: OpcodeID = OP_SWITCH_IMM;
    pub const OPCODE_LENGTH: usize = 4;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
jtable: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),
default: Fits::<isize, {OpcodeSize::Narrow}>::convert_back(stream.add(3).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
jtable: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),
default: Fits::<isize, {OpcodeSize::Wide16}>::convert_back(stream.add(3).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
jtable: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),
default: Fits::<isize, {OpcodeSize::Wide32}>::convert_back(stream.add(3).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_offset<F>(&mut self, value: isize, f: F)
        where F: FnOnce() -> isize {
            if self.is_wide32() { self.set_offset_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_offset_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_offset_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_offset_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: isize, f: F) 
        where F: FnOnce() -> isize {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<isize, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<isize, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<isize, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_jtable<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_jtable_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_jtable_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_jtable_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_jtable_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_default<F>(&mut self, value: isize, f: F)
        where F: FnOnce() -> isize {
            if self.is_wide32() { self.set_default_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_default_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_default_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_default_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: isize, f: F) 
        where F: FnOnce() -> isize {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<isize, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<isize, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<isize, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                3 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                3 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                3 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,jtable: VirtualRegister,default: isize,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<isize, {OpcodeSize::Narrow}>::check(offset),
    OpcodeSize::Wide16 => Fits::<isize, {OpcodeSize::Wide16}>::check(offset),
    OpcodeSize::Wide32 => Fits::<isize, {OpcodeSize::Wide32}>::check(offset)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(jtable),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(jtable),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(jtable)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<isize, {OpcodeSize::Narrow}>::check(default),
    OpcodeSize::Wide16 => Fits::<isize, {OpcodeSize::Wide16}>::check(default),
    OpcodeSize::Wide32 => Fits::<isize, {OpcodeSize::Wide32}>::check(default)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,jtable: VirtualRegister,default: isize,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,offset, src, jtable, default, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<isize, {OpcodeSize::Narrow}>::convert(offset)),
    OpcodeSize::Wide16 => gen.write(Fits::<isize, {OpcodeSize::Wide16}>::convert(offset)),
    OpcodeSize::Wide32 => gen.write(Fits::<isize, {OpcodeSize::Wide32}>::convert(offset))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(jtable)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(jtable)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(jtable))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<isize, {OpcodeSize::Narrow}>::convert(default)),
    OpcodeSize::Wide16 => gen.write(Fits::<isize, {OpcodeSize::Wide16}>::convert(default)),
    OpcodeSize::Wide32 => gen.write(Fits::<isize, {OpcodeSize::Wide32}>::convert(default))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,jtable: VirtualRegister,default: isize,) -> bool {
    Self::emit_impl::<SIZE>(gen, offset, src, jtable, default, )
}

pub fn emit(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,jtable: VirtualRegister,default: isize,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, offset, src, jtable, default, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,jtable: VirtualRegister,default: isize,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, offset, src, jtable, default, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,jtable: VirtualRegister,default: isize,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, offset, src, jtable, default, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,jtable: VirtualRegister,default: isize,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, offset, src, jtable, default, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,jtable: VirtualRegister,default: isize,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,offset, src, jtable, default, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, offset, src, jtable, default, ) {
            return;
        }
    }

    Self::emit_wide32(gen, offset, src, jtable, default, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**switch-imm"[2 - _size_shift_amount..]);
dumper.dump_operand("offset", self.offset, true);
dumper.dump_operand("src", self.src, false);
dumper.dump_operand("jtable", self.jtable, false);
dumper.dump_operand("default", self.default, false);

}

}

pub const OP_SWITCH_IMM_OFFSET_INDEX: usize = 0;
pub const OP_SWITCH_IMM_SRC_INDEX: usize = 1;
pub const OP_SWITCH_IMM_JTABLE_INDEX: usize = 2;
pub const OP_SWITCH_IMM_DEFAULT_INDEX: usize = 3;


/// Jump to `offset` if `src` is an heap-object that is bytevector-like

#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpJbytevectorlike {
    pub base: BaseInstruction,
    pub offset: isize,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpJbytevectorlike {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpJbytevectorlike {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpJbytevectorlike {
    pub const OPCODE_ID: OpcodeID = OP_JBYTEVECTORLIKE;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_offset<F>(&mut self, value: isize, f: F)
        where F: FnOnce() -> isize {
            if self.is_wide32() { self.set_offset_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_offset_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_offset_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_offset_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: isize, f: F) 
        where F: FnOnce() -> isize {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<isize, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<isize, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<isize, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<isize, {OpcodeSize::Narrow}>::check(offset),
    OpcodeSize::Wide16 => Fits::<isize, {OpcodeSize::Wide16}>::check(offset),
    OpcodeSize::Wide32 => Fits::<isize, {OpcodeSize::Wide32}>::check(offset)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,offset, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<isize, {OpcodeSize::Narrow}>::convert(offset)),
    OpcodeSize::Wide16 => gen.write(Fits::<isize, {OpcodeSize::Wide16}>::convert(offset)),
    OpcodeSize::Wide32 => gen.write(Fits::<isize, {OpcodeSize::Wide32}>::convert(offset))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, offset, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, offset, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, offset, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, offset, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, offset, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,offset, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, offset, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, offset, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**jbytevectorlike"[2 - _size_shift_amount..]);
dumper.dump_operand("offset", self.offset, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_JBYTEVECTORLIKE_OFFSET_INDEX: usize = 0;
pub const OP_JBYTEVECTORLIKE_SRC_INDEX: usize = 1;


/// Jump to `offset` if `src` is an heap-object that is vector-like

#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpJvectorlike {
    pub base: BaseInstruction,
    pub offset: isize,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpJvectorlike {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpJvectorlike {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpJvectorlike {
    pub const OPCODE_ID: OpcodeID = OP_JVECTORLIKE;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_offset<F>(&mut self, value: isize, f: F)
        where F: FnOnce() -> isize {
            if self.is_wide32() { self.set_offset_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_offset_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_offset_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_offset_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: isize, f: F) 
        where F: FnOnce() -> isize {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<isize, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<isize, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<isize, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<isize, {OpcodeSize::Narrow}>::check(offset),
    OpcodeSize::Wide16 => Fits::<isize, {OpcodeSize::Wide16}>::check(offset),
    OpcodeSize::Wide32 => Fits::<isize, {OpcodeSize::Wide32}>::check(offset)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,offset, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<isize, {OpcodeSize::Narrow}>::convert(offset)),
    OpcodeSize::Wide16 => gen.write(Fits::<isize, {OpcodeSize::Wide16}>::convert(offset)),
    OpcodeSize::Wide32 => gen.write(Fits::<isize, {OpcodeSize::Wide32}>::convert(offset))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, offset, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, offset, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, offset, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, offset, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, offset, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,offset, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, offset, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, offset, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**jvectorlike"[2 - _size_shift_amount..]);
dumper.dump_operand("offset", self.offset, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_JVECTORLIKE_OFFSET_INDEX: usize = 0;
pub const OP_JVECTORLIKE_SRC_INDEX: usize = 1;


/// Jump to `offset` if `src` is an heap-object with tag equal to `tag`
/// # Note
/// Does not check feature-bits of CellTag, only first 14 bits are checked

#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpJtaggedImm {
    pub base: BaseInstruction,
    pub offset: isize,
    pub src: VirtualRegister,
    pub tag: u16,
 
}
impl std::ops::Deref for OpJtaggedImm {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpJtaggedImm {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpJtaggedImm {
    pub const OPCODE_ID: OpcodeID = OP_JTAGGED_IMM;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
tag: Fits::<u16, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
tag: Fits::<u16, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
tag: Fits::<u16, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_offset<F>(&mut self, value: isize, f: F)
        where F: FnOnce() -> isize {
            if self.is_wide32() { self.set_offset_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_offset_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_offset_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_offset_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: isize, f: F) 
        where F: FnOnce() -> isize {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<isize, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<isize, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<isize, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_tag<F>(&mut self, value: u16, f: F)
        where F: FnOnce() -> u16 {
            if self.is_wide32() { self.set_tag_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_tag_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_tag_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_tag_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: u16, f: F) 
        where F: FnOnce() -> u16 {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<u16, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<u16, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<u16, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<u16, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<u16, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<u16, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,tag: u16,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<isize, {OpcodeSize::Narrow}>::check(offset),
    OpcodeSize::Wide16 => Fits::<isize, {OpcodeSize::Wide16}>::check(offset),
    OpcodeSize::Wide32 => Fits::<isize, {OpcodeSize::Wide32}>::check(offset)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<u16, {OpcodeSize::Narrow}>::check(tag),
    OpcodeSize::Wide16 => Fits::<u16, {OpcodeSize::Wide16}>::check(tag),
    OpcodeSize::Wide32 => Fits::<u16, {OpcodeSize::Wide32}>::check(tag)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,tag: u16,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,offset, src, tag, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<isize, {OpcodeSize::Narrow}>::convert(offset)),
    OpcodeSize::Wide16 => gen.write(Fits::<isize, {OpcodeSize::Wide16}>::convert(offset)),
    OpcodeSize::Wide32 => gen.write(Fits::<isize, {OpcodeSize::Wide32}>::convert(offset))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<u16, {OpcodeSize::Narrow}>::convert(tag)),
    OpcodeSize::Wide16 => gen.write(Fits::<u16, {OpcodeSize::Wide16}>::convert(tag)),
    OpcodeSize::Wide32 => gen.write(Fits::<u16, {OpcodeSize::Wide32}>::convert(tag))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,tag: u16,) -> bool {
    Self::emit_impl::<SIZE>(gen, offset, src, tag, )
}

pub fn emit(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,tag: u16,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, offset, src, tag, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,tag: u16,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, offset, src, tag, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,tag: u16,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, offset, src, tag, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,tag: u16,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, offset, src, tag, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,tag: u16,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,offset, src, tag, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, offset, src, tag, ) {
            return;
        }
    }

    Self::emit_wide32(gen, offset, src, tag, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**jtagged-imm"[2 - _size_shift_amount..]);
dumper.dump_operand("offset", self.offset, true);
dumper.dump_operand("src", self.src, false);
dumper.dump_operand("tag", self.tag, false);

}

}

pub const OP_JTAGGED_IMM_OFFSET_INDEX: usize = 0;
pub const OP_JTAGGED_IMM_SRC_INDEX: usize = 1;
pub const OP_JTAGGED_IMM_TAG_INDEX: usize = 2;


/// Jump to `offset` if `src` is an heap-object with tag equal to `tag`
/// # Note
/// Does not check feature-bits of CellTag, only first 14 bits are checked

#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpJtagged {
    pub base: BaseInstruction,
    pub offset: isize,
    pub src: VirtualRegister,
    pub tag: VirtualRegister,
 
}
impl std::ops::Deref for OpJtagged {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpJtagged {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpJtagged {
    pub const OPCODE_ID: OpcodeID = OP_JTAGGED;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
tag: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
tag: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
tag: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_offset<F>(&mut self, value: isize, f: F)
        where F: FnOnce() -> isize {
            if self.is_wide32() { self.set_offset_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_offset_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_offset_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_offset_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: isize, f: F) 
        where F: FnOnce() -> isize {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<isize, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<isize, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<isize, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_tag<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_tag_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_tag_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_tag_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_tag_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,tag: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<isize, {OpcodeSize::Narrow}>::check(offset),
    OpcodeSize::Wide16 => Fits::<isize, {OpcodeSize::Wide16}>::check(offset),
    OpcodeSize::Wide32 => Fits::<isize, {OpcodeSize::Wide32}>::check(offset)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(tag),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(tag),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(tag)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,tag: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,offset, src, tag, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<isize, {OpcodeSize::Narrow}>::convert(offset)),
    OpcodeSize::Wide16 => gen.write(Fits::<isize, {OpcodeSize::Wide16}>::convert(offset)),
    OpcodeSize::Wide32 => gen.write(Fits::<isize, {OpcodeSize::Wide32}>::convert(offset))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(tag)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(tag)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(tag))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,tag: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, offset, src, tag, )
}

pub fn emit(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,tag: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, offset, src, tag, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,tag: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, offset, src, tag, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,tag: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, offset, src, tag, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,tag: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, offset, src, tag, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,tag: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,offset, src, tag, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, offset, src, tag, ) {
            return;
        }
    }

    Self::emit_wide32(gen, offset, src, tag, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**jtagged"[2 - _size_shift_amount..]);
dumper.dump_operand("offset", self.offset, true);
dumper.dump_operand("src", self.src, false);
dumper.dump_operand("tag", self.tag, false);

}

}

pub const OP_JTAGGED_OFFSET_INDEX: usize = 0;
pub const OP_JTAGGED_SRC_INDEX: usize = 1;
pub const OP_JTAGGED_TAG_INDEX: usize = 2;


/// Checks if `src` is an heap-object that is bytevector-like, stores #t or #f in `dst`

#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsBytevectorlike {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpIsBytevectorlike {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsBytevectorlike {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsBytevectorlike {
    pub const OPCODE_ID: OpcodeID = OP_IS_BYTEVECTORLIKE;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-bytevectorlike"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_IS_BYTEVECTORLIKE_DST_INDEX: usize = 0;
pub const OP_IS_BYTEVECTORLIKE_SRC_INDEX: usize = 1;


/// Checks if `src` is an heap-object that is vector-like, stores #t or #f in `dst`

#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsVectorlike {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpIsVectorlike {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsVectorlike {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsVectorlike {
    pub const OPCODE_ID: OpcodeID = OP_IS_VECTORLIKE;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-vectorlike"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_IS_VECTORLIKE_DST_INDEX: usize = 0;
pub const OP_IS_VECTORLIKE_SRC_INDEX: usize = 1;


/// Fetches the tag of a heap object as SMI, feature bits are not preserved in SMI

#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpGetTagType {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpGetTagType {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpGetTagType {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpGetTagType {
    pub const OPCODE_ID: OpcodeID = OP_GET_TAG_TYPE;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**get-tag-type"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_GET_TAG_TYPE_DST_INDEX: usize = 0;
pub const OP_GET_TAG_TYPE_SRC_INDEX: usize = 1;


/// Fetches the tag of a heap object as SMI, feature bits are preserved in SMI

#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpGetTag {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpGetTag {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpGetTag {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpGetTag {
    pub const OPCODE_ID: OpcodeID = OP_GET_TAG;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**get-tag"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_GET_TAG_DST_INDEX: usize = 0;
pub const OP_GET_TAG_SRC_INDEX: usize = 1;


/// Tags heap object with `tag`

#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpTag {
    pub base: BaseInstruction,
    pub srcdst: VirtualRegister,
    pub tag: u16,
 
}
impl std::ops::Deref for OpTag {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpTag {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpTag {
    pub const OPCODE_ID: OpcodeID = OP_TAG;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
srcdst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
tag: Fits::<u16, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
srcdst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
tag: Fits::<u16, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
srcdst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
tag: Fits::<u16, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_srcdst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_srcdst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_srcdst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_srcdst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_srcdst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_tag<F>(&mut self, value: u16, f: F)
        where F: FnOnce() -> u16 {
            if self.is_wide32() { self.set_tag_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_tag_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_tag_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_tag_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: u16, f: F) 
        where F: FnOnce() -> u16 {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<u16, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<u16, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<u16, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<u16, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<u16, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<u16, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, srcdst: VirtualRegister,tag: u16,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(srcdst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(srcdst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(srcdst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<u16, {OpcodeSize::Narrow}>::check(tag),
    OpcodeSize::Wide16 => Fits::<u16, {OpcodeSize::Wide16}>::check(tag),
    OpcodeSize::Wide32 => Fits::<u16, {OpcodeSize::Wide32}>::check(tag)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, srcdst: VirtualRegister,tag: u16,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,srcdst, tag, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(srcdst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(srcdst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(srcdst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<u16, {OpcodeSize::Narrow}>::convert(tag)),
    OpcodeSize::Wide16 => gen.write(Fits::<u16, {OpcodeSize::Wide16}>::convert(tag)),
    OpcodeSize::Wide32 => gen.write(Fits::<u16, {OpcodeSize::Wide32}>::convert(tag))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, srcdst: VirtualRegister,tag: u16,) -> bool {
    Self::emit_impl::<SIZE>(gen, srcdst, tag, )
}

pub fn emit(gen: &mut BytecodeGenerator, srcdst: VirtualRegister,tag: u16,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, srcdst, tag, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, srcdst: VirtualRegister,tag: u16,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, srcdst, tag, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, srcdst: VirtualRegister,tag: u16,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, srcdst, tag, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, srcdst: VirtualRegister,tag: u16,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, srcdst, tag, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, srcdst: VirtualRegister,tag: u16,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,srcdst, tag, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, srcdst, tag, ) {
            return;
        }
    }

    Self::emit_wide32(gen, srcdst, tag, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**tag"[2 - _size_shift_amount..]);
dumper.dump_operand("srcdst", self.srcdst, true);
dumper.dump_operand("tag", self.tag, false);

}

}

pub const OP_TAG_SRCDST_INDEX: usize = 0;
pub const OP_TAG_TAG_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpJsmi {
    pub base: BaseInstruction,
    pub offset: isize,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpJsmi {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpJsmi {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpJsmi {
    pub const OPCODE_ID: OpcodeID = OP_JSMI;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_offset<F>(&mut self, value: isize, f: F)
        where F: FnOnce() -> isize {
            if self.is_wide32() { self.set_offset_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_offset_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_offset_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_offset_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: isize, f: F) 
        where F: FnOnce() -> isize {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<isize, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<isize, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<isize, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<isize, {OpcodeSize::Narrow}>::check(offset),
    OpcodeSize::Wide16 => Fits::<isize, {OpcodeSize::Wide16}>::check(offset),
    OpcodeSize::Wide32 => Fits::<isize, {OpcodeSize::Wide32}>::check(offset)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,offset, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<isize, {OpcodeSize::Narrow}>::convert(offset)),
    OpcodeSize::Wide16 => gen.write(Fits::<isize, {OpcodeSize::Wide16}>::convert(offset)),
    OpcodeSize::Wide32 => gen.write(Fits::<isize, {OpcodeSize::Wide32}>::convert(offset))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, offset, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, offset, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, offset, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, offset, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, offset, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,offset, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, offset, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, offset, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**jsmi"[2 - _size_shift_amount..]);
dumper.dump_operand("offset", self.offset, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_JSMI_OFFSET_INDEX: usize = 0;
pub const OP_JSMI_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpJnsmi {
    pub base: BaseInstruction,
    pub offset: isize,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpJnsmi {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpJnsmi {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpJnsmi {
    pub const OPCODE_ID: OpcodeID = OP_JNSMI;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_offset<F>(&mut self, value: isize, f: F)
        where F: FnOnce() -> isize {
            if self.is_wide32() { self.set_offset_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_offset_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_offset_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_offset_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: isize, f: F) 
        where F: FnOnce() -> isize {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<isize, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<isize, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<isize, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<isize, {OpcodeSize::Narrow}>::check(offset),
    OpcodeSize::Wide16 => Fits::<isize, {OpcodeSize::Wide16}>::check(offset),
    OpcodeSize::Wide32 => Fits::<isize, {OpcodeSize::Wide32}>::check(offset)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,offset, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<isize, {OpcodeSize::Narrow}>::convert(offset)),
    OpcodeSize::Wide16 => gen.write(Fits::<isize, {OpcodeSize::Wide16}>::convert(offset)),
    OpcodeSize::Wide32 => gen.write(Fits::<isize, {OpcodeSize::Wide32}>::convert(offset))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, offset, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, offset, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, offset, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, offset, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, offset, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,offset, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, offset, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, offset, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**jnsmi"[2 - _size_shift_amount..]);
dumper.dump_operand("offset", self.offset, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_JNSMI_OFFSET_INDEX: usize = 0;
pub const OP_JNSMI_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpJheapobject {
    pub base: BaseInstruction,
    pub offset: isize,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpJheapobject {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpJheapobject {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpJheapobject {
    pub const OPCODE_ID: OpcodeID = OP_JHEAPOBJECT;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_offset<F>(&mut self, value: isize, f: F)
        where F: FnOnce() -> isize {
            if self.is_wide32() { self.set_offset_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_offset_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_offset_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_offset_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: isize, f: F) 
        where F: FnOnce() -> isize {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<isize, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<isize, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<isize, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<isize, {OpcodeSize::Narrow}>::check(offset),
    OpcodeSize::Wide16 => Fits::<isize, {OpcodeSize::Wide16}>::check(offset),
    OpcodeSize::Wide32 => Fits::<isize, {OpcodeSize::Wide32}>::check(offset)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,offset, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<isize, {OpcodeSize::Narrow}>::convert(offset)),
    OpcodeSize::Wide16 => gen.write(Fits::<isize, {OpcodeSize::Wide16}>::convert(offset)),
    OpcodeSize::Wide32 => gen.write(Fits::<isize, {OpcodeSize::Wide32}>::convert(offset))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, offset, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, offset, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, offset, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, offset, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, offset, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,offset, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, offset, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, offset, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**jheapobject"[2 - _size_shift_amount..]);
dumper.dump_operand("offset", self.offset, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_JHEAPOBJECT_OFFSET_INDEX: usize = 0;
pub const OP_JHEAPOBJECT_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpJnheapobject {
    pub base: BaseInstruction,
    pub offset: isize,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpJnheapobject {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpJnheapobject {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpJnheapobject {
    pub const OPCODE_ID: OpcodeID = OP_JNHEAPOBJECT;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_offset<F>(&mut self, value: isize, f: F)
        where F: FnOnce() -> isize {
            if self.is_wide32() { self.set_offset_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_offset_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_offset_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_offset_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: isize, f: F) 
        where F: FnOnce() -> isize {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<isize, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<isize, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<isize, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<isize, {OpcodeSize::Narrow}>::check(offset),
    OpcodeSize::Wide16 => Fits::<isize, {OpcodeSize::Wide16}>::check(offset),
    OpcodeSize::Wide32 => Fits::<isize, {OpcodeSize::Wide32}>::check(offset)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,offset, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<isize, {OpcodeSize::Narrow}>::convert(offset)),
    OpcodeSize::Wide16 => gen.write(Fits::<isize, {OpcodeSize::Wide16}>::convert(offset)),
    OpcodeSize::Wide32 => gen.write(Fits::<isize, {OpcodeSize::Wide32}>::convert(offset))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, offset, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, offset, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, offset, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, offset, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, offset, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,offset, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, offset, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, offset, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**jnheapobject"[2 - _size_shift_amount..]);
dumper.dump_operand("offset", self.offset, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_JNHEAPOBJECT_OFFSET_INDEX: usize = 0;
pub const OP_JNHEAPOBJECT_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpJtrue {
    pub base: BaseInstruction,
    pub offset: isize,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpJtrue {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpJtrue {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpJtrue {
    pub const OPCODE_ID: OpcodeID = OP_JTRUE;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_offset<F>(&mut self, value: isize, f: F)
        where F: FnOnce() -> isize {
            if self.is_wide32() { self.set_offset_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_offset_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_offset_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_offset_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: isize, f: F) 
        where F: FnOnce() -> isize {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<isize, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<isize, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<isize, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<isize, {OpcodeSize::Narrow}>::check(offset),
    OpcodeSize::Wide16 => Fits::<isize, {OpcodeSize::Wide16}>::check(offset),
    OpcodeSize::Wide32 => Fits::<isize, {OpcodeSize::Wide32}>::check(offset)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,offset, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<isize, {OpcodeSize::Narrow}>::convert(offset)),
    OpcodeSize::Wide16 => gen.write(Fits::<isize, {OpcodeSize::Wide16}>::convert(offset)),
    OpcodeSize::Wide32 => gen.write(Fits::<isize, {OpcodeSize::Wide32}>::convert(offset))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, offset, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, offset, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, offset, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, offset, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, offset, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,offset, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, offset, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, offset, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**jtrue"[2 - _size_shift_amount..]);
dumper.dump_operand("offset", self.offset, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_JTRUE_OFFSET_INDEX: usize = 0;
pub const OP_JTRUE_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpJfalse {
    pub base: BaseInstruction,
    pub offset: isize,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpJfalse {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpJfalse {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpJfalse {
    pub const OPCODE_ID: OpcodeID = OP_JFALSE;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_offset<F>(&mut self, value: isize, f: F)
        where F: FnOnce() -> isize {
            if self.is_wide32() { self.set_offset_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_offset_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_offset_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_offset_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: isize, f: F) 
        where F: FnOnce() -> isize {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<isize, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<isize, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<isize, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<isize, {OpcodeSize::Narrow}>::check(offset),
    OpcodeSize::Wide16 => Fits::<isize, {OpcodeSize::Wide16}>::check(offset),
    OpcodeSize::Wide32 => Fits::<isize, {OpcodeSize::Wide32}>::check(offset)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,offset, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<isize, {OpcodeSize::Narrow}>::convert(offset)),
    OpcodeSize::Wide16 => gen.write(Fits::<isize, {OpcodeSize::Wide16}>::convert(offset)),
    OpcodeSize::Wide32 => gen.write(Fits::<isize, {OpcodeSize::Wide32}>::convert(offset))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, offset, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, offset, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, offset, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, offset, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, offset, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,offset, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, offset, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, offset, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**jfalse"[2 - _size_shift_amount..]);
dumper.dump_operand("offset", self.offset, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_JFALSE_OFFSET_INDEX: usize = 0;
pub const OP_JFALSE_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpJmp {
    pub base: BaseInstruction,
    pub offset: isize,
 
}
impl std::ops::Deref for OpJmp {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpJmp {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpJmp {
    pub const OPCODE_ID: OpcodeID = OP_JMP;
    pub const OPCODE_LENGTH: usize = 1;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
offset: Fits::<isize, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_offset<F>(&mut self, value: isize, f: F)
        where F: FnOnce() -> isize {
            if self.is_wide32() { self.set_offset_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_offset_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_offset_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_offset_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: isize, f: F) 
        where F: FnOnce() -> isize {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<isize, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<isize, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<isize, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<isize, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<isize, {OpcodeSize::Narrow}>::check(offset),
    OpcodeSize::Wide16 => Fits::<isize, {OpcodeSize::Wide16}>::check(offset),
    OpcodeSize::Wide32 => Fits::<isize, {OpcodeSize::Wide32}>::check(offset)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,offset, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<isize, {OpcodeSize::Narrow}>::convert(offset)),
    OpcodeSize::Wide16 => gen.write(Fits::<isize, {OpcodeSize::Wide16}>::convert(offset)),
    OpcodeSize::Wide32 => gen.write(Fits::<isize, {OpcodeSize::Wide32}>::convert(offset))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,) -> bool {
    Self::emit_impl::<SIZE>(gen, offset, )
}

pub fn emit(gen: &mut BytecodeGenerator, offset: isize,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, offset, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, offset: isize,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, offset, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, offset: isize,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, offset, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, offset: isize,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, offset, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, offset: isize,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,offset, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, offset, ) {
            return;
        }
    }

    Self::emit_wide32(gen, offset, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**jmp"[2 - _size_shift_amount..]);
dumper.dump_operand("offset", self.offset, true);

}

}

pub const OP_JMP_OFFSET_INDEX: usize = 0;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsFixnum {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpIsFixnum {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsFixnum {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsFixnum {
    pub const OPCODE_ID: OpcodeID = OP_IS_FIXNUM;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-fixnum"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_IS_FIXNUM_DST_INDEX: usize = 0;
pub const OP_IS_FIXNUM_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsFlonum {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpIsFlonum {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsFlonum {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsFlonum {
    pub const OPCODE_ID: OpcodeID = OP_IS_FLONUM;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-flonum"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_IS_FLONUM_DST_INDEX: usize = 0;
pub const OP_IS_FLONUM_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsExact {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpIsExact {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsExact {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsExact {
    pub const OPCODE_ID: OpcodeID = OP_IS_EXACT;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-exact"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_IS_EXACT_DST_INDEX: usize = 0;
pub const OP_IS_EXACT_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsInexact {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpIsInexact {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsInexact {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsInexact {
    pub const OPCODE_ID: OpcodeID = OP_IS_INEXACT;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-inexact"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_IS_INEXACT_DST_INDEX: usize = 0;
pub const OP_IS_INEXACT_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsNumber {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpIsNumber {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsNumber {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsNumber {
    pub const OPCODE_ID: OpcodeID = OP_IS_NUMBER;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-number"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_IS_NUMBER_DST_INDEX: usize = 0;
pub const OP_IS_NUMBER_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsReal {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpIsReal {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsReal {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsReal {
    pub const OPCODE_ID: OpcodeID = OP_IS_REAL;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-real"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_IS_REAL_DST_INDEX: usize = 0;
pub const OP_IS_REAL_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsComplex {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpIsComplex {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsComplex {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsComplex {
    pub const OPCODE_ID: OpcodeID = OP_IS_COMPLEX;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-complex"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_IS_COMPLEX_DST_INDEX: usize = 0;
pub const OP_IS_COMPLEX_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsRational {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpIsRational {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsRational {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsRational {
    pub const OPCODE_ID: OpcodeID = OP_IS_RATIONAL;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-rational"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_IS_RATIONAL_DST_INDEX: usize = 0;
pub const OP_IS_RATIONAL_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsInteger {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpIsInteger {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsInteger {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsInteger {
    pub const OPCODE_ID: OpcodeID = OP_IS_INTEGER;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-integer"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_IS_INTEGER_DST_INDEX: usize = 0;
pub const OP_IS_INTEGER_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsExactInteger {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpIsExactInteger {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsExactInteger {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsExactInteger {
    pub const OPCODE_ID: OpcodeID = OP_IS_EXACT_INTEGER;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-exact-integer"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_IS_EXACT_INTEGER_DST_INDEX: usize = 0;
pub const OP_IS_EXACT_INTEGER_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsExactNonnegativeInteger {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpIsExactNonnegativeInteger {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsExactNonnegativeInteger {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsExactNonnegativeInteger {
    pub const OPCODE_ID: OpcodeID = OP_IS_EXACT_NONNEGATIVE_INTEGER;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-exact-nonnegative-integer"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_IS_EXACT_NONNEGATIVE_INTEGER_DST_INDEX: usize = 0;
pub const OP_IS_EXACT_NONNEGATIVE_INTEGER_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsExactPositiveInteger {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpIsExactPositiveInteger {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsExactPositiveInteger {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsExactPositiveInteger {
    pub const OPCODE_ID: OpcodeID = OP_IS_EXACT_POSITIVE_INTEGER;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-exact-positive-integer"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_IS_EXACT_POSITIVE_INTEGER_DST_INDEX: usize = 0;
pub const OP_IS_EXACT_POSITIVE_INTEGER_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsInexactReal {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpIsInexactReal {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsInexactReal {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsInexactReal {
    pub const OPCODE_ID: OpcodeID = OP_IS_INEXACT_REAL;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-inexact-real"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_IS_INEXACT_REAL_DST_INDEX: usize = 0;
pub const OP_IS_INEXACT_REAL_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpEnter {
    pub base: BaseInstruction,
 
}
impl std::ops::Deref for OpEnter {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpEnter {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpEnter {
    pub const OPCODE_ID: OpcodeID = OP_ENTER;
    pub const OPCODE_LENGTH: usize = 0;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { Self { base: Default::default() } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { Self { base: Default::default() } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { Self { base: Default::default() } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, ) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, ) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
    
    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, ) -> bool {
    Self::emit_impl::<SIZE>(gen, )
}

pub fn emit(gen: &mut BytecodeGenerator, ) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, ) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, ) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, ) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, ) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, ) {
            return;
        }
    }

    Self::emit_wide32(gen, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**enter"[2 - _size_shift_amount..]);

}

}




#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpWide32 {
    pub base: BaseInstruction,
 
}
impl std::ops::Deref for OpWide32 {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpWide32 {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpWide32 {
    pub const OPCODE_ID: OpcodeID = OP_WIDE32;
    pub const OPCODE_LENGTH: usize = 0;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { Self { base: Default::default() } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { Self { base: Default::default() } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { Self { base: Default::default() } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, ) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, ) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
    
    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, ) -> bool {
    Self::emit_impl::<SIZE>(gen, )
}

pub fn emit(gen: &mut BytecodeGenerator, ) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, ) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, ) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, ) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, ) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, ) {
            return;
        }
    }

    Self::emit_wide32(gen, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**wide32"[2 - _size_shift_amount..]);

}

}




#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpWide16 {
    pub base: BaseInstruction,
 
}
impl std::ops::Deref for OpWide16 {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpWide16 {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpWide16 {
    pub const OPCODE_ID: OpcodeID = OP_WIDE16;
    pub const OPCODE_LENGTH: usize = 0;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { Self { base: Default::default() } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { Self { base: Default::default() } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { Self { base: Default::default() } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, ) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, ) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
    
    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, ) -> bool {
    Self::emit_impl::<SIZE>(gen, )
}

pub fn emit(gen: &mut BytecodeGenerator, ) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, ) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, ) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, ) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, ) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, ) {
            return;
        }
    }

    Self::emit_wide32(gen, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**wide16"[2 - _size_shift_amount..]);

}

}




#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpNop {
    pub base: BaseInstruction,
 
}
impl std::ops::Deref for OpNop {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpNop {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpNop {
    pub const OPCODE_ID: OpcodeID = OP_NOP;
    pub const OPCODE_LENGTH: usize = 0;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { Self { base: Default::default() } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { Self { base: Default::default() } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { Self { base: Default::default() } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, ) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, ) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
    
    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, ) -> bool {
    Self::emit_impl::<SIZE>(gen, )
}

pub fn emit(gen: &mut BytecodeGenerator, ) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, ) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, ) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, ) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, ) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, ) {
            return;
        }
    }

    Self::emit_wide32(gen, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**nop"[2 - _size_shift_amount..]);

}

}




#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpTailCall {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub callee: VirtualRegister,
    pub argc: usize,
    pub argv: usize,
 
}
impl std::ops::Deref for OpTailCall {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpTailCall {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpTailCall {
    pub const OPCODE_ID: OpcodeID = OP_TAIL_CALL;
    pub const OPCODE_LENGTH: usize = 4;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
callee: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
argc: Fits::<usize, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),
argv: Fits::<usize, {OpcodeSize::Narrow}>::convert_back(stream.add(3).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
callee: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
argc: Fits::<usize, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),
argv: Fits::<usize, {OpcodeSize::Wide16}>::convert_back(stream.add(3).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
callee: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
argc: Fits::<usize, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),
argv: Fits::<usize, {OpcodeSize::Wide32}>::convert_back(stream.add(3).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_callee<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_callee_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_callee_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_callee_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_callee_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_argc<F>(&mut self, value: usize, f: F)
        where F: FnOnce() -> usize {
            if self.is_wide32() { self.set_argc_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_argc_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_argc_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_argc_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: usize, f: F) 
        where F: FnOnce() -> usize {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<usize, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<usize, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<usize, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<usize, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<usize, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<usize, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_argv<F>(&mut self, value: usize, f: F)
        where F: FnOnce() -> usize {
            if self.is_wide32() { self.set_argv_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_argv_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_argv_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_argv_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: usize, f: F) 
        where F: FnOnce() -> usize {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<usize, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<usize, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<usize, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                3 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<usize, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                3 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<usize, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                3 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<usize, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,callee: VirtualRegister,argc: usize,argv: usize,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(callee),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(callee),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(callee)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<usize, {OpcodeSize::Narrow}>::check(argc),
    OpcodeSize::Wide16 => Fits::<usize, {OpcodeSize::Wide16}>::check(argc),
    OpcodeSize::Wide32 => Fits::<usize, {OpcodeSize::Wide32}>::check(argc)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<usize, {OpcodeSize::Narrow}>::check(argv),
    OpcodeSize::Wide16 => Fits::<usize, {OpcodeSize::Wide16}>::check(argv),
    OpcodeSize::Wide32 => Fits::<usize, {OpcodeSize::Wide32}>::check(argv)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,callee: VirtualRegister,argc: usize,argv: usize,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, callee, argc, argv, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(callee)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(callee)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(callee))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<usize, {OpcodeSize::Narrow}>::convert(argc)),
    OpcodeSize::Wide16 => gen.write(Fits::<usize, {OpcodeSize::Wide16}>::convert(argc)),
    OpcodeSize::Wide32 => gen.write(Fits::<usize, {OpcodeSize::Wide32}>::convert(argc))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<usize, {OpcodeSize::Narrow}>::convert(argv)),
    OpcodeSize::Wide16 => gen.write(Fits::<usize, {OpcodeSize::Wide16}>::convert(argv)),
    OpcodeSize::Wide32 => gen.write(Fits::<usize, {OpcodeSize::Wide32}>::convert(argv))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,callee: VirtualRegister,argc: usize,argv: usize,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, callee, argc, argv, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,callee: VirtualRegister,argc: usize,argv: usize,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, callee, argc, argv, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,callee: VirtualRegister,argc: usize,argv: usize,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, callee, argc, argv, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,callee: VirtualRegister,argc: usize,argv: usize,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, callee, argc, argv, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,callee: VirtualRegister,argc: usize,argv: usize,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, callee, argc, argv, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,callee: VirtualRegister,argc: usize,argv: usize,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, callee, argc, argv, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, callee, argc, argv, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, callee, argc, argv, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**tail-call"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("callee", self.callee, false);
dumper.dump_operand("argc", self.argc, false);
dumper.dump_operand("argv", self.argv, false);

}

}

pub const OP_TAIL_CALL_DST_INDEX: usize = 0;
pub const OP_TAIL_CALL_CALLEE_INDEX: usize = 1;
pub const OP_TAIL_CALL_ARGC_INDEX: usize = 2;
pub const OP_TAIL_CALL_ARGV_INDEX: usize = 3;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpCall {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub callee: VirtualRegister,
    pub argc: usize,
    pub argv: usize,
 
}
impl std::ops::Deref for OpCall {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpCall {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpCall {
    pub const OPCODE_ID: OpcodeID = OP_CALL;
    pub const OPCODE_LENGTH: usize = 4;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
callee: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
argc: Fits::<usize, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),
argv: Fits::<usize, {OpcodeSize::Narrow}>::convert_back(stream.add(3).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
callee: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
argc: Fits::<usize, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),
argv: Fits::<usize, {OpcodeSize::Wide16}>::convert_back(stream.add(3).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
callee: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
argc: Fits::<usize, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),
argv: Fits::<usize, {OpcodeSize::Wide32}>::convert_back(stream.add(3).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_callee<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_callee_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_callee_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_callee_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_callee_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_argc<F>(&mut self, value: usize, f: F)
        where F: FnOnce() -> usize {
            if self.is_wide32() { self.set_argc_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_argc_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_argc_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_argc_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: usize, f: F) 
        where F: FnOnce() -> usize {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<usize, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<usize, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<usize, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<usize, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<usize, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<usize, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_argv<F>(&mut self, value: usize, f: F)
        where F: FnOnce() -> usize {
            if self.is_wide32() { self.set_argv_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_argv_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_argv_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_argv_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: usize, f: F) 
        where F: FnOnce() -> usize {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<usize, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<usize, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<usize, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                3 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<usize, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                3 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<usize, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                3 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<usize, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,callee: VirtualRegister,argc: usize,argv: usize,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(callee),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(callee),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(callee)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<usize, {OpcodeSize::Narrow}>::check(argc),
    OpcodeSize::Wide16 => Fits::<usize, {OpcodeSize::Wide16}>::check(argc),
    OpcodeSize::Wide32 => Fits::<usize, {OpcodeSize::Wide32}>::check(argc)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<usize, {OpcodeSize::Narrow}>::check(argv),
    OpcodeSize::Wide16 => Fits::<usize, {OpcodeSize::Wide16}>::check(argv),
    OpcodeSize::Wide32 => Fits::<usize, {OpcodeSize::Wide32}>::check(argv)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,callee: VirtualRegister,argc: usize,argv: usize,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, callee, argc, argv, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(callee)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(callee)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(callee))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<usize, {OpcodeSize::Narrow}>::convert(argc)),
    OpcodeSize::Wide16 => gen.write(Fits::<usize, {OpcodeSize::Wide16}>::convert(argc)),
    OpcodeSize::Wide32 => gen.write(Fits::<usize, {OpcodeSize::Wide32}>::convert(argc))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<usize, {OpcodeSize::Narrow}>::convert(argv)),
    OpcodeSize::Wide16 => gen.write(Fits::<usize, {OpcodeSize::Wide16}>::convert(argv)),
    OpcodeSize::Wide32 => gen.write(Fits::<usize, {OpcodeSize::Wide32}>::convert(argv))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,callee: VirtualRegister,argc: usize,argv: usize,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, callee, argc, argv, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,callee: VirtualRegister,argc: usize,argv: usize,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, callee, argc, argv, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,callee: VirtualRegister,argc: usize,argv: usize,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, callee, argc, argv, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,callee: VirtualRegister,argc: usize,argv: usize,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, callee, argc, argv, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,callee: VirtualRegister,argc: usize,argv: usize,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, callee, argc, argv, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,callee: VirtualRegister,argc: usize,argv: usize,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, callee, argc, argv, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, callee, argc, argv, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, callee, argc, argv, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**call"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("callee", self.callee, false);
dumper.dump_operand("argc", self.argc, false);
dumper.dump_operand("argv", self.argv, false);

}

}

pub const OP_CALL_DST_INDEX: usize = 0;
pub const OP_CALL_CALLEE_INDEX: usize = 1;
pub const OP_CALL_ARGC_INDEX: usize = 2;
pub const OP_CALL_ARGV_INDEX: usize = 3;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpRet {
    pub base: BaseInstruction,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpRet {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpRet {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpRet {
    pub const OPCODE_ID: OpcodeID = OP_RET;
    pub const OPCODE_LENGTH: usize = 1;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**ret"[2 - _size_shift_amount..]);
dumper.dump_operand("src", self.src, true);

}

}

pub const OP_RET_SRC_INDEX: usize = 0;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpMovSmi {
    pub base: BaseInstruction,
    pub dest: VirtualRegister,
    pub imm: i32,
 
}
impl std::ops::Deref for OpMovSmi {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpMovSmi {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpMovSmi {
    pub const OPCODE_ID: OpcodeID = OP_MOV_SMI;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dest: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
imm: Fits::<i32, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dest: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
imm: Fits::<i32, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dest: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
imm: Fits::<i32, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dest<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dest_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dest_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dest_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dest_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_imm<F>(&mut self, value: i32, f: F)
        where F: FnOnce() -> i32 {
            if self.is_wide32() { self.set_imm_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_imm_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_imm_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_imm_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: i32, f: F) 
        where F: FnOnce() -> i32 {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<i32, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<i32, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<i32, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<i32, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<i32, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<i32, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dest: VirtualRegister,imm: i32,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dest),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dest),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dest)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<i32, {OpcodeSize::Narrow}>::check(imm),
    OpcodeSize::Wide16 => Fits::<i32, {OpcodeSize::Wide16}>::check(imm),
    OpcodeSize::Wide32 => Fits::<i32, {OpcodeSize::Wide32}>::check(imm)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dest: VirtualRegister,imm: i32,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dest, imm, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dest)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dest)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dest))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<i32, {OpcodeSize::Narrow}>::convert(imm)),
    OpcodeSize::Wide16 => gen.write(Fits::<i32, {OpcodeSize::Wide16}>::convert(imm)),
    OpcodeSize::Wide32 => gen.write(Fits::<i32, {OpcodeSize::Wide32}>::convert(imm))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dest: VirtualRegister,imm: i32,) -> bool {
    Self::emit_impl::<SIZE>(gen, dest, imm, )
}

pub fn emit(gen: &mut BytecodeGenerator, dest: VirtualRegister,imm: i32,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dest, imm, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dest: VirtualRegister,imm: i32,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dest, imm, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dest: VirtualRegister,imm: i32,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dest, imm, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dest: VirtualRegister,imm: i32,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dest, imm, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dest: VirtualRegister,imm: i32,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dest, imm, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dest, imm, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dest, imm, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**mov-smi"[2 - _size_shift_amount..]);
dumper.dump_operand("dest", self.dest, true);
dumper.dump_operand("imm", self.imm, false);

}

}

pub const OP_MOV_SMI_DEST_INDEX: usize = 0;
pub const OP_MOV_SMI_IMM_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpMov {
    pub base: BaseInstruction,
    pub dest: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpMov {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpMov {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpMov {
    pub const OPCODE_ID: OpcodeID = OP_MOV;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dest: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dest: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dest: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dest<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dest_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dest_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dest_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dest_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dest: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dest),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dest),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dest)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dest: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dest, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dest)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dest)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dest))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dest: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dest, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dest: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dest, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dest: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dest, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dest: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dest, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dest: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dest, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dest: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dest, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dest, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dest, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**mov"[2 - _size_shift_amount..]);
dumper.dump_operand("dest", self.dest, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_MOV_DEST_INDEX: usize = 0;
pub const OP_MOV_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpTypeOf {
    pub base: BaseInstruction,
    pub dest: VirtualRegister,
    pub src: VirtualRegister,
 
}
impl std::ops::Deref for OpTypeOf {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpTypeOf {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpTypeOf {
    pub const OPCODE_ID: OpcodeID = OP_TYPE_OF;
    pub const OPCODE_LENGTH: usize = 2;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dest: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dest: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dest: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
src: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dest<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dest_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dest_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dest_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dest_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_src<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_src_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_src_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_src_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_src_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dest: VirtualRegister,src: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dest),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dest),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dest)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(src),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(src),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(src)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dest: VirtualRegister,src: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dest, src, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dest)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dest)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dest))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(src)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(src)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(src))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dest: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dest, src, )
}

pub fn emit(gen: &mut BytecodeGenerator, dest: VirtualRegister,src: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dest, src, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dest: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dest, src, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dest: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dest, src, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dest: VirtualRegister,src: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dest, src, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dest: VirtualRegister,src: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dest, src, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dest, src, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dest, src, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**type-of"[2 - _size_shift_amount..]);
dumper.dump_operand("dest", self.dest, true);
dumper.dump_operand("src", self.src, false);

}

}

pub const OP_TYPE_OF_DEST_INDEX: usize = 0;
pub const OP_TYPE_OF_SRC_INDEX: usize = 1;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsNumericallyGreater {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub lhs: VirtualRegister,
    pub rhs: VirtualRegister,
 
}
impl std::ops::Deref for OpIsNumericallyGreater {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsNumericallyGreater {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsNumericallyGreater {
    pub const OPCODE_ID: OpcodeID = OP_IS_NUMERICALLY_GREATER;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_lhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_lhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_lhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_lhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_lhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_rhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_rhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_rhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_rhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_rhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(lhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(lhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(lhs)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(rhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(rhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(rhs)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, lhs, rhs, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(lhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(lhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(lhs))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(rhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(rhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(rhs))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, lhs, rhs, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, lhs, rhs, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, lhs, rhs, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, lhs, rhs, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, lhs, rhs, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-numerically-greater"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("lhs", self.lhs, false);
dumper.dump_operand("rhs", self.rhs, false);

}

}

pub const OP_IS_NUMERICALLY_GREATER_DST_INDEX: usize = 0;
pub const OP_IS_NUMERICALLY_GREATER_LHS_INDEX: usize = 1;
pub const OP_IS_NUMERICALLY_GREATER_RHS_INDEX: usize = 2;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsNumericallyLesseq {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub lhs: VirtualRegister,
    pub rhs: VirtualRegister,
 
}
impl std::ops::Deref for OpIsNumericallyLesseq {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsNumericallyLesseq {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsNumericallyLesseq {
    pub const OPCODE_ID: OpcodeID = OP_IS_NUMERICALLY_LESSEQ;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_lhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_lhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_lhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_lhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_lhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_rhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_rhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_rhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_rhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_rhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(lhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(lhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(lhs)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(rhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(rhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(rhs)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, lhs, rhs, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(lhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(lhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(lhs))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(rhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(rhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(rhs))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, lhs, rhs, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, lhs, rhs, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, lhs, rhs, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, lhs, rhs, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, lhs, rhs, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-numerically-lesseq"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("lhs", self.lhs, false);
dumper.dump_operand("rhs", self.rhs, false);

}

}

pub const OP_IS_NUMERICALLY_LESSEQ_DST_INDEX: usize = 0;
pub const OP_IS_NUMERICALLY_LESSEQ_LHS_INDEX: usize = 1;
pub const OP_IS_NUMERICALLY_LESSEQ_RHS_INDEX: usize = 2;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsNumericallyLess {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub lhs: VirtualRegister,
    pub rhs: VirtualRegister,
 
}
impl std::ops::Deref for OpIsNumericallyLess {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsNumericallyLess {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsNumericallyLess {
    pub const OPCODE_ID: OpcodeID = OP_IS_NUMERICALLY_LESS;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_lhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_lhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_lhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_lhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_lhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_rhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_rhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_rhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_rhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_rhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(lhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(lhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(lhs)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(rhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(rhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(rhs)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, lhs, rhs, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(lhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(lhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(lhs))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(rhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(rhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(rhs))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, lhs, rhs, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, lhs, rhs, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, lhs, rhs, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, lhs, rhs, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, lhs, rhs, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-numerically-less"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("lhs", self.lhs, false);
dumper.dump_operand("rhs", self.rhs, false);

}

}

pub const OP_IS_NUMERICALLY_LESS_DST_INDEX: usize = 0;
pub const OP_IS_NUMERICALLY_LESS_LHS_INDEX: usize = 1;
pub const OP_IS_NUMERICALLY_LESS_RHS_INDEX: usize = 2;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsNumericallyGreatereq {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub lhs: VirtualRegister,
    pub rhs: VirtualRegister,
 
}
impl std::ops::Deref for OpIsNumericallyGreatereq {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsNumericallyGreatereq {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsNumericallyGreatereq {
    pub const OPCODE_ID: OpcodeID = OP_IS_NUMERICALLY_GREATEREQ;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_lhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_lhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_lhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_lhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_lhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_rhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_rhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_rhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_rhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_rhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(lhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(lhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(lhs)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(rhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(rhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(rhs)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, lhs, rhs, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(lhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(lhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(lhs))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(rhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(rhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(rhs))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, lhs, rhs, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, lhs, rhs, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, lhs, rhs, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, lhs, rhs, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, lhs, rhs, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-numerically-greatereq"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("lhs", self.lhs, false);
dumper.dump_operand("rhs", self.rhs, false);

}

}

pub const OP_IS_NUMERICALLY_GREATEREQ_DST_INDEX: usize = 0;
pub const OP_IS_NUMERICALLY_GREATEREQ_LHS_INDEX: usize = 1;
pub const OP_IS_NUMERICALLY_GREATEREQ_RHS_INDEX: usize = 2;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsNumericallyEqual {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub lhs: VirtualRegister,
    pub rhs: VirtualRegister,
 
}
impl std::ops::Deref for OpIsNumericallyEqual {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsNumericallyEqual {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsNumericallyEqual {
    pub const OPCODE_ID: OpcodeID = OP_IS_NUMERICALLY_EQUAL;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_lhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_lhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_lhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_lhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_lhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_rhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_rhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_rhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_rhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_rhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(lhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(lhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(lhs)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(rhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(rhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(rhs)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, lhs, rhs, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(lhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(lhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(lhs))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(rhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(rhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(rhs))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, lhs, rhs, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, lhs, rhs, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, lhs, rhs, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, lhs, rhs, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, lhs, rhs, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-numerically-equal"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("lhs", self.lhs, false);
dumper.dump_operand("rhs", self.rhs, false);

}

}

pub const OP_IS_NUMERICALLY_EQUAL_DST_INDEX: usize = 0;
pub const OP_IS_NUMERICALLY_EQUAL_LHS_INDEX: usize = 1;
pub const OP_IS_NUMERICALLY_EQUAL_RHS_INDEX: usize = 2;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsEqual {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub lhs: VirtualRegister,
    pub rhs: VirtualRegister,
 
}
impl std::ops::Deref for OpIsEqual {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsEqual {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsEqual {
    pub const OPCODE_ID: OpcodeID = OP_IS_EQUAL;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_lhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_lhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_lhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_lhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_lhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_rhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_rhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_rhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_rhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_rhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(lhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(lhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(lhs)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(rhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(rhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(rhs)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, lhs, rhs, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(lhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(lhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(lhs))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(rhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(rhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(rhs))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, lhs, rhs, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, lhs, rhs, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, lhs, rhs, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, lhs, rhs, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, lhs, rhs, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-equal"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("lhs", self.lhs, false);
dumper.dump_operand("rhs", self.rhs, false);

}

}

pub const OP_IS_EQUAL_DST_INDEX: usize = 0;
pub const OP_IS_EQUAL_LHS_INDEX: usize = 1;
pub const OP_IS_EQUAL_RHS_INDEX: usize = 2;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsEqv {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub lhs: VirtualRegister,
    pub rhs: VirtualRegister,
 
}
impl std::ops::Deref for OpIsEqv {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsEqv {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsEqv {
    pub const OPCODE_ID: OpcodeID = OP_IS_EQV;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_lhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_lhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_lhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_lhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_lhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_rhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_rhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_rhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_rhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_rhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(lhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(lhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(lhs)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(rhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(rhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(rhs)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, lhs, rhs, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(lhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(lhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(lhs))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(rhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(rhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(rhs))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, lhs, rhs, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, lhs, rhs, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, lhs, rhs, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, lhs, rhs, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, lhs, rhs, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-eqv"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("lhs", self.lhs, false);
dumper.dump_operand("rhs", self.rhs, false);

}

}

pub const OP_IS_EQV_DST_INDEX: usize = 0;
pub const OP_IS_EQV_LHS_INDEX: usize = 1;
pub const OP_IS_EQV_RHS_INDEX: usize = 2;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsEq {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub lhs: VirtualRegister,
    pub rhs: VirtualRegister,
 
}
impl std::ops::Deref for OpIsEq {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsEq {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsEq {
    pub const OPCODE_ID: OpcodeID = OP_IS_EQ;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_lhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_lhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_lhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_lhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_lhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_rhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_rhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_rhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_rhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_rhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(lhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(lhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(lhs)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(rhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(rhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(rhs)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, lhs, rhs, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(lhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(lhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(lhs))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(rhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(rhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(rhs))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, lhs, rhs, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, lhs, rhs, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, lhs, rhs, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, lhs, rhs, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, lhs, rhs, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-eq"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("lhs", self.lhs, false);
dumper.dump_operand("rhs", self.rhs, false);

}

}

pub const OP_IS_EQ_DST_INDEX: usize = 0;
pub const OP_IS_EQ_LHS_INDEX: usize = 1;
pub const OP_IS_EQ_RHS_INDEX: usize = 2;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsStringEqual {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub lhs: VirtualRegister,
    pub rhs: VirtualRegister,
 
}
impl std::ops::Deref for OpIsStringEqual {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsStringEqual {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsStringEqual {
    pub const OPCODE_ID: OpcodeID = OP_IS_STRING_EQUAL;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_lhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_lhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_lhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_lhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_lhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_rhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_rhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_rhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_rhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_rhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(lhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(lhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(lhs)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(rhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(rhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(rhs)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, lhs, rhs, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(lhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(lhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(lhs))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(rhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(rhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(rhs))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, lhs, rhs, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, lhs, rhs, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, lhs, rhs, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, lhs, rhs, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, lhs, rhs, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-string-equal"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("lhs", self.lhs, false);
dumper.dump_operand("rhs", self.rhs, false);

}

}

pub const OP_IS_STRING_EQUAL_DST_INDEX: usize = 0;
pub const OP_IS_STRING_EQUAL_LHS_INDEX: usize = 1;
pub const OP_IS_STRING_EQUAL_RHS_INDEX: usize = 2;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsStringLesseq {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub lhs: VirtualRegister,
    pub rhs: VirtualRegister,
 
}
impl std::ops::Deref for OpIsStringLesseq {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsStringLesseq {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsStringLesseq {
    pub const OPCODE_ID: OpcodeID = OP_IS_STRING_LESSEQ;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_lhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_lhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_lhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_lhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_lhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_rhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_rhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_rhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_rhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_rhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(lhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(lhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(lhs)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(rhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(rhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(rhs)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, lhs, rhs, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(lhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(lhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(lhs))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(rhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(rhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(rhs))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, lhs, rhs, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, lhs, rhs, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, lhs, rhs, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, lhs, rhs, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, lhs, rhs, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-string-lesseq"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("lhs", self.lhs, false);
dumper.dump_operand("rhs", self.rhs, false);

}

}

pub const OP_IS_STRING_LESSEQ_DST_INDEX: usize = 0;
pub const OP_IS_STRING_LESSEQ_LHS_INDEX: usize = 1;
pub const OP_IS_STRING_LESSEQ_RHS_INDEX: usize = 2;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsStringGreatereq {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub lhs: VirtualRegister,
    pub rhs: VirtualRegister,
 
}
impl std::ops::Deref for OpIsStringGreatereq {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsStringGreatereq {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsStringGreatereq {
    pub const OPCODE_ID: OpcodeID = OP_IS_STRING_GREATEREQ;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_lhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_lhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_lhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_lhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_lhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_rhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_rhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_rhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_rhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_rhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(lhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(lhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(lhs)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(rhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(rhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(rhs)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, lhs, rhs, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(lhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(lhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(lhs))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(rhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(rhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(rhs))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, lhs, rhs, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, lhs, rhs, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, lhs, rhs, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, lhs, rhs, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, lhs, rhs, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-string-greatereq"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("lhs", self.lhs, false);
dumper.dump_operand("rhs", self.rhs, false);

}

}

pub const OP_IS_STRING_GREATEREQ_DST_INDEX: usize = 0;
pub const OP_IS_STRING_GREATEREQ_LHS_INDEX: usize = 1;
pub const OP_IS_STRING_GREATEREQ_RHS_INDEX: usize = 2;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsStringGreater {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub lhs: VirtualRegister,
    pub rhs: VirtualRegister,
 
}
impl std::ops::Deref for OpIsStringGreater {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsStringGreater {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsStringGreater {
    pub const OPCODE_ID: OpcodeID = OP_IS_STRING_GREATER;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_lhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_lhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_lhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_lhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_lhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_rhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_rhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_rhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_rhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_rhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(lhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(lhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(lhs)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(rhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(rhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(rhs)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, lhs, rhs, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(lhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(lhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(lhs))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(rhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(rhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(rhs))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, lhs, rhs, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, lhs, rhs, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, lhs, rhs, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, lhs, rhs, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, lhs, rhs, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-string-greater"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("lhs", self.lhs, false);
dumper.dump_operand("rhs", self.rhs, false);

}

}

pub const OP_IS_STRING_GREATER_DST_INDEX: usize = 0;
pub const OP_IS_STRING_GREATER_LHS_INDEX: usize = 1;
pub const OP_IS_STRING_GREATER_RHS_INDEX: usize = 2;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsStringLess {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub lhs: VirtualRegister,
    pub rhs: VirtualRegister,
 
}
impl std::ops::Deref for OpIsStringLess {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsStringLess {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsStringLess {
    pub const OPCODE_ID: OpcodeID = OP_IS_STRING_LESS;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_lhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_lhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_lhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_lhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_lhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_rhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_rhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_rhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_rhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_rhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(lhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(lhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(lhs)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(rhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(rhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(rhs)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, lhs, rhs, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(lhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(lhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(lhs))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(rhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(rhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(rhs))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, lhs, rhs, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, lhs, rhs, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, lhs, rhs, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, lhs, rhs, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, lhs, rhs, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-string-less"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("lhs", self.lhs, false);
dumper.dump_operand("rhs", self.rhs, false);

}

}

pub const OP_IS_STRING_LESS_DST_INDEX: usize = 0;
pub const OP_IS_STRING_LESS_LHS_INDEX: usize = 1;
pub const OP_IS_STRING_LESS_RHS_INDEX: usize = 2;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsCharEqual {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub lhs: VirtualRegister,
    pub rhs: VirtualRegister,
 
}
impl std::ops::Deref for OpIsCharEqual {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsCharEqual {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsCharEqual {
    pub const OPCODE_ID: OpcodeID = OP_IS_CHAR_EQUAL;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_lhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_lhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_lhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_lhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_lhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_rhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_rhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_rhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_rhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_rhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(lhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(lhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(lhs)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(rhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(rhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(rhs)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, lhs, rhs, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(lhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(lhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(lhs))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(rhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(rhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(rhs))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, lhs, rhs, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, lhs, rhs, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, lhs, rhs, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, lhs, rhs, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, lhs, rhs, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-char-equal"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("lhs", self.lhs, false);
dumper.dump_operand("rhs", self.rhs, false);

}

}

pub const OP_IS_CHAR_EQUAL_DST_INDEX: usize = 0;
pub const OP_IS_CHAR_EQUAL_LHS_INDEX: usize = 1;
pub const OP_IS_CHAR_EQUAL_RHS_INDEX: usize = 2;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsCharLesseq {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub lhs: VirtualRegister,
    pub rhs: VirtualRegister,
 
}
impl std::ops::Deref for OpIsCharLesseq {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsCharLesseq {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsCharLesseq {
    pub const OPCODE_ID: OpcodeID = OP_IS_CHAR_LESSEQ;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_lhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_lhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_lhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_lhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_lhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_rhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_rhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_rhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_rhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_rhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(lhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(lhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(lhs)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(rhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(rhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(rhs)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, lhs, rhs, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(lhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(lhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(lhs))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(rhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(rhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(rhs))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, lhs, rhs, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, lhs, rhs, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, lhs, rhs, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, lhs, rhs, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, lhs, rhs, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-char-lesseq"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("lhs", self.lhs, false);
dumper.dump_operand("rhs", self.rhs, false);

}

}

pub const OP_IS_CHAR_LESSEQ_DST_INDEX: usize = 0;
pub const OP_IS_CHAR_LESSEQ_LHS_INDEX: usize = 1;
pub const OP_IS_CHAR_LESSEQ_RHS_INDEX: usize = 2;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsCharGreatereq {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub lhs: VirtualRegister,
    pub rhs: VirtualRegister,
 
}
impl std::ops::Deref for OpIsCharGreatereq {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsCharGreatereq {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsCharGreatereq {
    pub const OPCODE_ID: OpcodeID = OP_IS_CHAR_GREATEREQ;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_lhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_lhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_lhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_lhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_lhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_rhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_rhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_rhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_rhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_rhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(lhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(lhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(lhs)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(rhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(rhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(rhs)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, lhs, rhs, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(lhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(lhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(lhs))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(rhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(rhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(rhs))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, lhs, rhs, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, lhs, rhs, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, lhs, rhs, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, lhs, rhs, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, lhs, rhs, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-char-greatereq"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("lhs", self.lhs, false);
dumper.dump_operand("rhs", self.rhs, false);

}

}

pub const OP_IS_CHAR_GREATEREQ_DST_INDEX: usize = 0;
pub const OP_IS_CHAR_GREATEREQ_LHS_INDEX: usize = 1;
pub const OP_IS_CHAR_GREATEREQ_RHS_INDEX: usize = 2;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsCharGreater {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub lhs: VirtualRegister,
    pub rhs: VirtualRegister,
 
}
impl std::ops::Deref for OpIsCharGreater {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsCharGreater {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsCharGreater {
    pub const OPCODE_ID: OpcodeID = OP_IS_CHAR_GREATER;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_lhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_lhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_lhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_lhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_lhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_rhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_rhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_rhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_rhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_rhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(lhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(lhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(lhs)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(rhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(rhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(rhs)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, lhs, rhs, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(lhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(lhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(lhs))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(rhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(rhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(rhs))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, lhs, rhs, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, lhs, rhs, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, lhs, rhs, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, lhs, rhs, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, lhs, rhs, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-char-greater"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("lhs", self.lhs, false);
dumper.dump_operand("rhs", self.rhs, false);

}

}

pub const OP_IS_CHAR_GREATER_DST_INDEX: usize = 0;
pub const OP_IS_CHAR_GREATER_LHS_INDEX: usize = 1;
pub const OP_IS_CHAR_GREATER_RHS_INDEX: usize = 2;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsCharLess {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub lhs: VirtualRegister,
    pub rhs: VirtualRegister,
 
}
impl std::ops::Deref for OpIsCharLess {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsCharLess {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsCharLess {
    pub const OPCODE_ID: OpcodeID = OP_IS_CHAR_LESS;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_lhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_lhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_lhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_lhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_lhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_rhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_rhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_rhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_rhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_rhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(lhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(lhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(lhs)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(rhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(rhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(rhs)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, lhs, rhs, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(lhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(lhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(lhs))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(rhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(rhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(rhs))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, lhs, rhs, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, lhs, rhs, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, lhs, rhs, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, lhs, rhs, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, lhs, rhs, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-char-less"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("lhs", self.lhs, false);
dumper.dump_operand("rhs", self.rhs, false);

}

}

pub const OP_IS_CHAR_LESS_DST_INDEX: usize = 0;
pub const OP_IS_CHAR_LESS_LHS_INDEX: usize = 1;
pub const OP_IS_CHAR_LESS_RHS_INDEX: usize = 2;



#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct OpIsBytevectorEqual {
    pub base: BaseInstruction,
    pub dst: VirtualRegister,
    pub lhs: VirtualRegister,
    pub rhs: VirtualRegister,
 
}
impl std::ops::Deref for OpIsBytevectorEqual {
    type Target = BaseInstruction;
    fn deref(&self) -> &Self::Target {
        &self.base
    }
}

impl std::ops::DerefMut for OpIsBytevectorEqual {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.base
    }
}


impl OpIsBytevectorEqual {
    pub const OPCODE_ID: OpcodeID = OP_IS_BYTEVECTOR_EQUAL;
    pub const OPCODE_LENGTH: usize = 3;
    
    pub unsafe fn new_narrow(stream: *const u8) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide16(stream: *const u16) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert_back(stream.add(2).read_unaligned() as _),

 } }
    pub unsafe fn new_wide32(stream: *const u32) -> Self { 
         Self { 
base: Default::default(),
dst: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(0).read_unaligned() as _),
lhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(1).read_unaligned() as _),
rhs: Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert_back(stream.add(2).read_unaligned() as _),

 } }

    pub unsafe fn decode(stream: *const u8) -> Self {
        if *stream == OP_WIDE32 {
            return Self::new_wide32(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE).cast());
        } else if *stream == OP_WIDE16 {
            return Self::new_wide16(
                stream.add(1 + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE).cast());
        } else {
            return Self::new_narrow(
                stream.add(OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE))
        }
    }

    
        pub fn set_dst<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_dst_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_dst_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_dst_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_dst_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                0 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_lhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_lhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_lhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_lhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_lhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                1 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

        pub fn set_rhs<F>(&mut self, value: VirtualRegister, f: F)
        where F: FnOnce() -> VirtualRegister {
            if self.is_wide32() { self.set_rhs_sized::<{OpcodeSize::Wide32}, F>(value, f) }
            else if self.is_wide16() { self.set_rhs_sized::<{OpcodeSize::Wide16}, F>(value, f) }
            else { self.set_rhs_sized::<{OpcodeSize::Narrow}, F>(value, f) }
        }
        pub fn set_rhs_sized<const SIZE: OpcodeSize, F>(&mut self, mut value: VirtualRegister, f: F) 
        where F: FnOnce() -> VirtualRegister {
            if !(match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(value),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(value),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(value)
})
 {
                value = f();
            }
            unsafe {
                match SIZE {
                    OpcodeSize::Narrow => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Narrow}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Narrow}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Narrow}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(value) as _);
                    }

                    OpcodeSize::Wide16 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide16}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide16}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide16}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(value) as _);
                    }

                    OpcodeSize::Wide32 => {
                        let stream = (self as *mut Self as *mut u8)
                            .add(
                                2 * SIZE as usize 
                                + PaddingBySize::<{OpcodeSize::Wide32}>::VALUE
                                + OpcodeIDWidthBySize::<{OpcodeSize::Wide32}>::VALUE) as *mut <TypeBySize::<{OpcodeSize::Wide32}> as TypeBySizeTrait>::UnsignedType;
                        stream.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(value) as _);
                    }
                }
            }
        }
            
            
        

    
fn check_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
let _ = gen;
true
 && (match SIZE {
    OpcodeSize::Narrow => Fits::<OpcodeID, {OpcodeSize::Narrow}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide16 => Fits::<OpcodeID, {OpcodeSize::Wide16}>::check(Self::OPCODE_ID),
    OpcodeSize::Wide32 => Fits::<OpcodeID, {OpcodeSize::Wide32}>::check(Self::OPCODE_ID)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(dst),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(dst),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(dst)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(lhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(lhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(lhs)
})

 && (match SIZE {
    OpcodeSize::Narrow => Fits::<VirtualRegister, {OpcodeSize::Narrow}>::check(rhs),
    OpcodeSize::Wide16 => Fits::<VirtualRegister, {OpcodeSize::Wide16}>::check(rhs),
    OpcodeSize::Wide32 => Fits::<VirtualRegister, {OpcodeSize::Wide32}>::check(rhs)
})
    
}

    
fn emit_impl<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
if SIZE == OpcodeSize::Wide16 {
    gen.align_wide_opcode16();
} else if SIZE == OpcodeSize::Wide32 {
    gen.align_wide_opcode32();
}

if Self::check_impl::<{SIZE}>(gen,dst, lhs, rhs, ) {
    gen.record_opcode(Self::OPCODE_ID);
    if SIZE == OpcodeSize::Wide16 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE16));
    } else if SIZE == OpcodeSize::Wide32 {
        gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(OP_WIDE32));
    }
    gen.write(Fits::<OpcodeID, {OpcodeSize::Narrow}>::convert(Self::OPCODE_ID));
            match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(dst)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(dst)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(dst))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(lhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(lhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(lhs))
}
        match SIZE {
    OpcodeSize::Narrow => gen.write(Fits::<VirtualRegister, {OpcodeSize::Narrow}>::convert(rhs)),
    OpcodeSize::Wide16 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide16}>::convert(rhs)),
    OpcodeSize::Wide32 => gen.write(Fits::<VirtualRegister, {OpcodeSize::Wide32}>::convert(rhs))
}

    return true;
}

false 
}

    
pub fn emit_generic<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<SIZE>(gen, dst, lhs, rhs, )
}

pub fn emit(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    Self::emit_with_smallest_size_requirement::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, );
}

pub fn emit_narrow(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Narrow}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide16(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide16}>(gen, dst, lhs, rhs, )
}

pub fn emit_wide32(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) -> bool {
    Self::emit_impl::<{OpcodeSize::Wide32}>(gen, dst, lhs, rhs, )
}

pub fn emit_with_smallest_size_requirement<const SIZE: OpcodeSize>(gen: &mut BytecodeGenerator, dst: VirtualRegister,lhs: VirtualRegister,rhs: VirtualRegister,) {
    
    if SIZE as usize <= OpcodeSize::Narrow as usize {
        if Self::emit_narrow(gen,dst, lhs, rhs, ) {
            return;
        }
    }

    if SIZE as usize <= OpcodeSize::Wide16 as usize {
        if Self::emit_wide16(gen, dst, lhs, rhs, ) {
            return;
        }
    }

    Self::emit_wide32(gen, dst, lhs, rhs, );
}

    pub fn dump(&self, dumper: &mut BytecodeDumper, _location: usize, _size_shift_amount: usize) {
dumper.print_location_and_op(_location, &"**is-bytevector-equal"[2 - _size_shift_amount..]);
dumper.dump_operand("dst", self.dst, true);
dumper.dump_operand("lhs", self.lhs, false);
dumper.dump_operand("rhs", self.rhs, false);

}

}

pub const OP_IS_BYTEVECTOR_EQUAL_DST_INDEX: usize = 0;
pub const OP_IS_BYTEVECTOR_EQUAL_LHS_INDEX: usize = 1;
pub const OP_IS_BYTEVECTOR_EQUAL_RHS_INDEX: usize = 2;

pub unsafe fn raw_dump_bytecode(dumper: &mut BytecodeDumper, _location: usize, instruction: *const BaseInstruction) {
    match (*instruction).opcode_id() {
OP_BOX_FLONUM => (OpBoxFlonum::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_UNBOX_FLONUM => (OpUnboxFlonum::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_HEAP_OBJECT => (OpIsHeapObject::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_SMI => (OpIsSmi::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_ALLOCATE_UNTAGGED_KNOWN => (OpAllocateUntaggedKnown::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_ALLOCATE_UNTAGGED => (OpAllocateUntagged::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_ALLOCATE_KNOWN => (OpAllocateKnown::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_ALLOCATE => (OpAllocate::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_SCM_SET_BY_ID => (OpScmSetById::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_SCM_REF_BY_ID => (OpScmRefById::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_SCM_SET => (OpScmSet::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_SCM_REF => (OpScmRef::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_SCM_SET_WORD_BY_ID => (OpScmSetWordById::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_SCM_REF_WORD_BY_ID => (OpScmRefWordById::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_SCM_SET_WORD => (OpScmSetWord::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_SCM_REF_WORD => (OpScmRefWord::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_PAIR => (OpIsPair::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_NULL => (OpIsNull::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_VECTOR => (OpIsVector::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_STRING => (OpIsString::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_SYMBOL => (OpIsSymbol::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_TUPLE => (OpIsTuple::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_PROCEDURE => (OpIsProcedure::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_BOOLEAN => (OpIsBoolean::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_CHAR => (OpIsChar::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_UNDEFINED => (OpIsUndefined::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_UNSPECIFIED => (OpIsUnspecified::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_EOF_OBJECT => (OpIsEofObject::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_SWITCH_IMM => (OpSwitchImm::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_JBYTEVECTORLIKE => (OpJbytevectorlike::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_JVECTORLIKE => (OpJvectorlike::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_JTAGGED_IMM => (OpJtaggedImm::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_JTAGGED => (OpJtagged::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_BYTEVECTORLIKE => (OpIsBytevectorlike::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_VECTORLIKE => (OpIsVectorlike::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_GET_TAG_TYPE => (OpGetTagType::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_GET_TAG => (OpGetTag::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_TAG => (OpTag::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_JSMI => (OpJsmi::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_JNSMI => (OpJnsmi::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_JHEAPOBJECT => (OpJheapobject::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_JNHEAPOBJECT => (OpJnheapobject::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_JTRUE => (OpJtrue::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_JFALSE => (OpJfalse::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_JMP => (OpJmp::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_FIXNUM => (OpIsFixnum::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_FLONUM => (OpIsFlonum::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_EXACT => (OpIsExact::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_INEXACT => (OpIsInexact::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_NUMBER => (OpIsNumber::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_REAL => (OpIsReal::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_COMPLEX => (OpIsComplex::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_RATIONAL => (OpIsRational::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_INTEGER => (OpIsInteger::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_EXACT_INTEGER => (OpIsExactInteger::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_EXACT_NONNEGATIVE_INTEGER => (OpIsExactNonnegativeInteger::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_EXACT_POSITIVE_INTEGER => (OpIsExactPositiveInteger::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_INEXACT_REAL => (OpIsInexactReal::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_ENTER => (OpEnter::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_WIDE32 => (OpWide32::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_WIDE16 => (OpWide16::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_NOP => (OpNop::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_TAIL_CALL => (OpTailCall::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_CALL => (OpCall::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_RET => (OpRet::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_MOV_SMI => (OpMovSmi::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_MOV => (OpMov::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_TYPE_OF => (OpTypeOf::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_NUMERICALLY_GREATER => (OpIsNumericallyGreater::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_NUMERICALLY_LESSEQ => (OpIsNumericallyLesseq::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_NUMERICALLY_LESS => (OpIsNumericallyLess::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_NUMERICALLY_GREATEREQ => (OpIsNumericallyGreatereq::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_NUMERICALLY_EQUAL => (OpIsNumericallyEqual::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_EQUAL => (OpIsEqual::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_EQV => (OpIsEqv::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_EQ => (OpIsEq::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_STRING_EQUAL => (OpIsStringEqual::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_STRING_LESSEQ => (OpIsStringLesseq::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_STRING_GREATEREQ => (OpIsStringGreatereq::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_STRING_GREATER => (OpIsStringGreater::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_STRING_LESS => (OpIsStringLess::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_CHAR_EQUAL => (OpIsCharEqual::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_CHAR_LESSEQ => (OpIsCharLesseq::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_CHAR_GREATEREQ => (OpIsCharGreatereq::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_CHAR_GREATER => (OpIsCharGreater::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_CHAR_LESS => (OpIsCharLess::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),
OP_IS_BYTEVECTOR_EQUAL => (OpIsBytevectorEqual::decode(instruction as _)).dump(dumper, _location, (*instruction).size_shift_amount()),

x => unreachable!("unknown opcode: {}", x)
    }   
}
